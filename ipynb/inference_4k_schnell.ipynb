{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pipeline_flux import FluxPipeline\n",
    "from transformer_flux import FluxTransformer2DModel\n",
    "from attention_processor import FluxAttnAdaptationProcessor2_0\n",
    "from safetensors.torch import load_file, save_file\n",
    "from patch_conv import convert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfl_repo=\"black-forest-labs/FLUX.1-schnell\"\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.bfloat16\n",
    "transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=\"transformer\")\n",
    "pipe = FluxPipeline.from_pretrained(bfl_repo, transformer=transformer, torch_dtype=dtype)\n",
    "pipe.scheduler.config.use_dynamic_shifting = False\n",
    "pipe.scheduler.config.time_shift = 10\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4K model is based on 2K LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('ckpt/urae_2k_adapter.safetensors'):\n",
    "    hf_hub_download(repo_id=\"Huage001/URAE\", filename='urae_2k_adapter.safetensors', local_dir='ckpt', local_dir_use_symlinks=False)\n",
    "pipe.load_lora_weights(\"ckpt/urae_2k_adapter.safetensors\")\n",
    "pipe.fuse_lora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Substitute original attention processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 16\n",
    "attn_processors = {}\n",
    "for k in pipe.transformer.attn_processors.keys():\n",
    "    attn_processors[k] = FluxAttnAdaptationProcessor2_0(rank=rank, to_out='single' not in k).to(device, dtype)\n",
    "pipe.transformer.set_attn_processor(attn_processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If no cached major components, compute them via SVD and save them to cache_path\n",
    "* If you don't want to save cached major components, simply set `cache_path = None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = 'ckpt/_urae_4k_adapter_schnell.safetensors'\n",
    "if cache_path is not None and os.path.exists(cache_path):\n",
    "    pipe.transformer.to(dtype=dtype)\n",
    "    pipe.transformer.load_state_dict(load_file(cache_path), strict=False)\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(pipe.transformer.transformer_blocks)):\n",
    "            matrix_w = pipe.transformer.transformer_blocks[idx].attn.to_q.weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.transformer_blocks[idx].attn.to_q.weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_q_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_q_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "\n",
    "            matrix_w = pipe.transformer.transformer_blocks[idx].attn.to_k.weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.transformer_blocks[idx].attn.to_k.weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_k_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_k_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "\n",
    "            matrix_w = pipe.transformer.transformer_blocks[idx].attn.to_v.weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.transformer_blocks[idx].attn.to_v.weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_v_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_v_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "\n",
    "            matrix_w = pipe.transformer.transformer_blocks[idx].attn.to_out[0].weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.transformer_blocks[idx].attn.to_out[0].weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_out_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.transformer_blocks[idx].attn.processor.to_out_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "        for idx in range(len(pipe.transformer.single_transformer_blocks)):\n",
    "            matrix_w = pipe.transformer.single_transformer_blocks[idx].attn.to_q.weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.to_q.weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.processor.to_q_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.processor.to_q_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "\n",
    "            matrix_w = pipe.transformer.single_transformer_blocks[idx].attn.to_k.weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.to_k.weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.processor.to_k_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.processor.to_k_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "\n",
    "            matrix_w = pipe.transformer.single_transformer_blocks[idx].attn.to_v.weight.data.to(device)\n",
    "            matrix_u, matrix_s, matrix_v = torch.linalg.svd(matrix_w)\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.to_v.weight.data = (\n",
    "                matrix_u[:, :-rank] @ torch.diag(matrix_s[:-rank]) @ matrix_v[:-rank, :]\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.processor.to_v_b.weight.data = (\n",
    "                matrix_u[:, -rank:] @ torch.diag(torch.sqrt(matrix_s[-rank:]))\n",
    "            ).to('cpu')\n",
    "            pipe.transformer.single_transformer_blocks[idx].attn.processor.to_v_a.weight.data = (\n",
    "                torch.diag(torch.sqrt(matrix_s[-rank:])) @ matrix_v[-rank:, :]\n",
    "            ).to('cpu')\n",
    "    pipe.transformer.to(dtype=dtype)\n",
    "    if cache_path is not None:\n",
    "        state_dict = pipe.transformer.state_dict()\n",
    "        attn_state_dict = {}\n",
    "        for k in state_dict.keys():\n",
    "            if 'base_layer' in k:\n",
    "                attn_state_dict[k] = state_dict[k]\n",
    "        save_file(attn_state_dict, cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download pre-trained 4k adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('ckpt/urae_4k_adapter.safetensors'):\n",
    "    hf_hub_download(repo_id=\"Huage001/URAE\", filename='urae_4k_adapter.safetensors', local_dir='ckpt', local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optionally, you can convert the minor-component adapter into a LoRA for easier use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_conversion = True\n",
    "if lora_conversion and not os.path.exists('ckpt/urae_4k_adapter_lora_conversion_schnell.safetensors'):\n",
    "    cur = pipe.transformer.state_dict()\n",
    "    tgt = load_file('ckpt/urae_4k_adapter.safetensors')\n",
    "    ref = load_file('ckpt/urae_2k_adapter.safetensors')\n",
    "    new_ckpt = {}\n",
    "    for k in tgt.keys():\n",
    "        if 'to_k_a' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_k_a', '.to_k.lora_A')\n",
    "        elif 'to_k_b' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_k_b', '.to_k.lora_B')\n",
    "        elif 'to_q_a' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_q_a', '.to_q.lora_A')\n",
    "        elif 'to_q_b' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_q_b', '.to_q.lora_B')\n",
    "        elif 'to_v_a' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_v_a', '.to_v.lora_A')\n",
    "        elif 'to_v_b' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_v_b', '.to_v.lora_B')\n",
    "        elif 'to_out_a' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_out_a', '.to_out.0.lora_A')\n",
    "        elif 'to_out_b' in k:\n",
    "            k_ = 'transformer.' + k.replace('.processor.to_out_b', '.to_out.0.lora_B')\n",
    "        else:\n",
    "            print(k)\n",
    "            assert False\n",
    "        if '_a.' in k and '_b.' not in k:\n",
    "            new_ckpt[k_] = torch.cat([-cur[k], tgt[k], ref[k_]], dim=0)\n",
    "        elif '_b.' in k and '_a.' not in k:\n",
    "            new_ckpt[k_] = torch.cat([cur[k], tgt[k], ref[k_]], dim=1)\n",
    "        else:\n",
    "            print(k)\n",
    "            assert False\n",
    "    save_file(new_ckpt, 'ckpt/urae_4k_adapter_lora_conversion_schnell.safetensors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load state_dict of 4k adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, u = pipe.transformer.load_state_dict(load_file('ckpt/urae_4k_adapter.safetensors'), strict=False)\n",
    "assert len(u) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use patch-wise convolution for VAE to avoid OOM error when decoding\n",
    "* If still OOM, try replacing the following line with `pipe.vae.enable_tiling()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.vae = convert_model(pipe.vae, splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Everything ready. Let's generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A serene woman in a flowing azure dress, gracefully perched on a sunlit cliff overlooking a tranquil sea, her hair gently tousled by the breeze. The scene is infused with a sense of peace, evoking a dreamlike atmosphere, reminiscent of Impressionist paintings.\"\n",
    "height = 4096\n",
    "width = 4096\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    guidance_scale=0,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,\n",
    "    generator=torch.manual_seed(8888),\n",
    "    ntk_factor=10,\n",
    "    proportional_attention=True\n",
    ").images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
